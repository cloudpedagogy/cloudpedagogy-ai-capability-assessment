# AI Capability Self-Assessment Tool

A lightweight, static, browser-based self-assessment application for reflecting on organisational AI capability across six domains, based on the **CloudPedagogy AI Capability Framework**.

This tool is designed for use in education, research, and public-service contexts. It supports reflective discussion, sense-making, and capability development rather than auditing or benchmarking.

---

## Overview

The AI Capability Self-Assessment helps individuals and teams explore how well their organisation is currently positioned to use artificial intelligence responsibly, effectively, and sustainably.

Rather than focusing on specific AI tools, the assessment examines capability across six interdependent domains:

- **Awareness**
- **Human–AI Co-Agency**
- **Applied Practice & Innovation**
- **Ethics, Equity & Impact**
- **Decision-Making & Governance**
- **Reflection, Learning & Renewal**

### Outputs

The assessment provides:

- Domain-level scores and capability bands  
- A visual capability profile (radar and bar charts)  
- Derived indicators  
- Narrative summaries, priority actions, and risks (where applicable)

---

## Design Principles

This application is intentionally:

- **Framework-led** – grounded in the CloudPedagogy AI Capability Framework  
- **Reflective** – encourages judgement and discussion, not compliance  
- **Non-prescriptive** – avoids tool-specific or vendor-specific guidance  
- **Static and private** – runs entirely in the browser with no backend  
- **Explainable** – rule-based interpretation rather than opaque scoring  

No data is stored, transmitted, or tracked.

---

## How the Assessment Works

### 1. Question Set

Users respond to 24 statements (4 per domain), selecting one of four maturity levels (e.g. *Not in place* → *Embedded and reviewed*).

### 2. Scoring

Responses are converted into domain scores (0–100) and mapped to qualitative capability bands (e.g. *Emerging*, *Developing*, *Established*).

### 3. Interpretation Engine

A transparent, rule-based engine interprets patterns across domains to generate:

- Narrative summaries  
- Prioritised actions  
- Key risks (where relevant)  

Multiple rules can apply simultaneously, allowing nuanced interpretations for mixed profiles.

### 4. Visualisation

Results are displayed using simple radar and bar charts to support discussion and sense-making.

---

## Intended Use

This tool can be used for:

- Individual reflection by educators, researchers, or leaders  
- Team-based discussions or workshops  
- Capability-building conversations  
- Early-stage organisational sense-checks  

It is particularly suited to:

- Higher education institutions  
- Research organisations  
- Public and third-sector bodies  

---

## What This Tool Is Not

❌ Not an audit or compliance tool  
❌ Not a benchmark against other organisations  
❌ Not a maturity model tied to specific technologies  
❌ Not a diagnostic of individual performance  

The outputs should be treated as starting points for discussion, not definitive judgements.

